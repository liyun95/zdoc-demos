{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation QA using Zilliz Cloud and LlamaIndex\n",
    "\n",
    "This guide demonstrates how to integrate LlamaIndex with Zilliz Cloud to retrieve information from designated sources.\n",
    "\n",
    "## Before you start\n",
    "\n",
    "For this example, we are going to be using pymilvus to connect to use Zilliz Cloud and llama-index to handle the data manipulation and pipelining. This example will also require having an OpenAI API key for embedding generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pymilvus llama-index llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "We are going to use git to pull the Milvus website data. A majority of the documents come in the form of markdown files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/milvus-io/milvus-docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Here we can find the main parameters that need to be modified for running with your own accounts. Beside each is a description of what it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "from glob import glob\n",
    "from llama_index import download_loader, VectorStoreIndex, ServiceContext\n",
    "from llama_index.vector_stores import MilvusVectorStore\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "\n",
    "# Set up arguments\n",
    "\n",
    "# 1. Set up the name of the collection to be created.\n",
    "COLLECTION_NAME = 'document_qa_db'\n",
    "\n",
    "# 2. Set up the dimension of the embeddings.\n",
    "DIMENSION = 1536\n",
    "\n",
    "# 3. Set the inference parameters\n",
    "BATCH_SIZE = 128\n",
    "TOP_K = 3\n",
    "\n",
    "# 4. Set up the connection parameters for your Zilliz Cloud cluster.\n",
    "URI = 'YOUR_CLUSTER_ENDPOINT'\n",
    "\n",
    "TOKEN = 'YOUR_CLUSTER_TOKEN'\n",
    "\n",
    "# OpenAI API key\n",
    "environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consume Knowledge\n",
    "\n",
    "Once we have our data on the system we can proceed to consume it using LlamaIndex and upload it to Zilliz Cloud. This comes in the form of two steps. We begin by loading in a markdown reader from Llama Hub and converting all our markdowns into documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1613\n"
     ]
    }
   ],
   "source": [
    "# Load the markdown reader from the hub\n",
    "MarkdownReader = download_loader(\"MarkdownReader\")\n",
    "markdownreader = MarkdownReader()\n",
    "\n",
    "# Grab all markdown files and convert them using the reader\n",
    "docs = []\n",
    "for file in glob(\"../../../repos/milvus-docs/site/en/**/*.md\", recursive=True):\n",
    "    docs.extend(markdownreader.load_data(file=file))\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our documents formed, we can proceed to push them through into Zilliz Cloud. This step requires the configs for both Zilliz Cloud and OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthony/Documents/Github/zdoc-demos/python/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing documents into nodes: 100%|██████████| 1613/1613 [00:00<00:00, 2635.43it/s]\n",
      "Generating embeddings: 100%|██████████| 1716/1716 [05:26<00:00,  5.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# Push all markdown files into Zilliz Cloud\n",
    "vector_store = MilvusVectorStore(\n",
    "    uri=URI, \n",
    "    token=TOKEN, \n",
    "    collection_name=COLLECTION_NAME, \n",
    "    similarity_metric=\"L2\",\n",
    "    dim=DIMENSION,\n",
    ")\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=docs, \n",
    "    service_context=service_context,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask question\n",
    "\n",
    "With our documents loaded into Zilliz Cloud, we can begin asking questions. The questions will be searched against the knowledge base and any relevant documents will be used to generate an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IVF_FLAT is an index used in Milvus that divides vector space into list clusters. It compares the distances between the target vector and the centroids of all the clusters to return the nearest clusters. Then, it compares the distances between the target vector and the vectors in the selected clusters to find the nearest vectors. IVF_FLAT has performance advantages over FLAT when the number of vectors exceeds a certain threshold.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is IVF_FLAT?\")\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
