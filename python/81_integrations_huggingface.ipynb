{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering Using Zilliz Cloud and Hugging Face\n",
    "\n",
    "This page illustrates how to build a question-answering system using Zilliz Cloud as the vector database and Hugging Face as the embedding system.\n",
    "\n",
    "## Before you start\n",
    "\n",
    "The code snippets on this page require the following packages to be installed: **pymilvus**, **transformers****, and datasets**. The packages **transformers** and **datasets** are the Hugging Face packages to create the pipeline, and **pymilvus** is the client for Zilliz Cloud. If these packages are not present on your system, run the following commands to install them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets pymilvus torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you need to load the modules to be used in this guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PyMilvus in development\n",
    "# Should be replaced with `from pymilvus import *` in production\n",
    "# from pathlib import Path\n",
    "# import sys\n",
    "# sys.path.append(str(Path(\"/Users/anthony/Documents/play/refine_milvus/pymilvus\")))\n",
    "\n",
    "from pymilvus import connections, DataType, CollectionSchema, FieldSchema, Collection, utility\n",
    "from datasets import load_dataset_builder, load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch import clamp, sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Here we can find the parameters used in the following snippets. Some of them need to be changed to fit your environment. Beside each is a description of what it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up arguments\n",
    "\n",
    "# 1. Set the name of a dataset available on HuggingFace.\n",
    "DATASET = 'squad' \n",
    "\n",
    "# 2. Set parameters for the generation of a subset of the dataset.\n",
    "MODEL = 'bert-base-cased'\n",
    "TOKENIZATION_BATCH_SIZE = 1000\n",
    "INFERENCE_BATCH_SIZE = 64\n",
    "INSERT_RATIO = 0.01\n",
    "\n",
    "# 3. Set up the name of the collection to be created.\n",
    "COLLECTION_NAME = 'huggingface_db'\n",
    "\n",
    "# 4. Set up the dimension of the embeddings.\n",
    "DIMENSION = 768\n",
    "\n",
    "# 5. Set the number of records to return.\n",
    "LIMIT = 10\n",
    "\n",
    "# 6. Set up the connection parameters for your Zilliz Cloud cluster.\n",
    "URI = 'https://in03-24426a264d9129a.api.gcp-us-west1.zillizcloud.com'\n",
    "TOKEN = 'e90b8dfdf09dce948b254f0d238324143e65ced62e469aed3bfe56c474d39d3f0a0e270c599ab6acb23ffe1aa2972d1adf6d4827'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know more about the model and dataset used on this page, refer to [bert-base-uncased](https://huggingface.co/bert-base-uncased) and [squad](https://huggingface.co/datasets/squad).\n",
    "\n",
    "## Create a collection\n",
    "\n",
    "This section deals with Zilliz Cloud and setting up the cluster for this use case. Within Zilliz Cloud, we need to set up a collection and index it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Status(code=0, message=)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to Zilliz Cloud and create a collection\n",
    "\n",
    "connections.connect(\n",
    "    alias='default',\n",
    "    # Public endpoint obtained from Zilliz Cloud\n",
    "    uri=URI,\n",
    "    token=TOKEN\n",
    ")\n",
    "\n",
    "if COLLECTION_NAME in utility.list_collections():\n",
    "    utility.drop_collection(COLLECTION_NAME)\n",
    "\n",
    "fields = [\n",
    "    FieldSchema(name='id', dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name='original_question', dtype=DataType.VARCHAR, max_length=1000),\n",
    "    FieldSchema(name='answer', dtype=DataType.VARCHAR, max_length=1000),\n",
    "    FieldSchema(name='original_question_embedding', dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)\n",
    "]\n",
    "\n",
    "schema = CollectionSchema(fields=fields)\n",
    "\n",
    "collection = Collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    schema=schema,\n",
    ")\n",
    "\n",
    "index_params = {\n",
    "    'metric_type': 'L2',\n",
    "    'index_type': 'AUTOINDEX',\n",
    "    'params': {'nlist': 1024}\n",
    "}\n",
    "\n",
    "collection.create_index(\n",
    "    field_name='original_question_embedding',\n",
    "    index_params=index_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert data\n",
    "\n",
    "Once we have the collection set up, we need to start inserting our data. This is done in three steps\n",
    "\n",
    "- tokenizing the original question,\n",
    "\n",
    "- embedding the tokenized question, and\n",
    "\n",
    "- inserting the embedding, original question, and answer.\n",
    "\n",
    "In this example, the data includes the original question, the original question's embedding, and the answer to the original question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function insert_function at 0x1525cc1f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map: 100%|██████████| 982/982 [00:08<00:00, 117.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and extract a subset\n",
    "\n",
    "data_dataset = load_dataset(DATASET, split='all')\n",
    "data_dataset = data_dataset.train_test_split(test_size=INSERT_RATIO, seed=42)['test']\n",
    "data_dataset = data_dataset.map(\n",
    "    lambda val: {'answer': val['answers']['text'][0]}, \n",
    "    remove_columns=['answers']\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# Tokenize the question into the format that BERT takes\n",
    "def tokenize_question(batch):\n",
    "    results = tokenizer(\n",
    "        batch['question'],\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        padding = \"max_length\", \n",
    "        return_attention_mask = True, \n",
    "        return_tensors = \"pt\"\n",
    "    )\n",
    "\n",
    "    batch['input_ids'] = results['input_ids']\n",
    "    batch['token_type_ids'] = results['token_type_ids']\n",
    "    batch['attention_mask'] = results['attention_mask']\n",
    "\n",
    "    return batch\n",
    "\n",
    "# Generate the tokens for each entry\n",
    "data_dataset = data_dataset.map(\n",
    "    tokenize_question, \n",
    "    batched=True, \n",
    "    batch_size=TOKENIZATION_BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Set the output format to torch so it can be pushed into embedding model\n",
    "data_dataset.set_format(\n",
    "    type='torch', \n",
    "    columns=['input_ids', 'token_type_ids', 'attention_mask'],\n",
    "    output_all_columns=True\n",
    ")\n",
    "\n",
    "# Embed the tokenized question and take the mean pool with respect to attention mask of hidden layer\n",
    "\n",
    "model = AutoModel.from_pretrained(MODEL)\n",
    "\n",
    "def embed(batch):\n",
    "    sentence_embs = model(\n",
    "        input_ids=batch['input_ids'],\n",
    "        token_type_ids=batch['token_type_ids'],\n",
    "        attention_mask=batch['attention_mask']\n",
    "    )[0]\n",
    "    input_mask_expanded = batch['attention_mask'].unsqueeze(-1).expand(sentence_embs.size()).float()\n",
    "    batch['question_embedding'] = sum(sentence_embs * input_mask_expanded, 1) / clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return batch\n",
    "\n",
    "data_dataset = data_dataset.map(\n",
    "    embed, \n",
    "    batched=True, \n",
    "    batch_size=INFERENCE_BATCH_SIZE,\n",
    "    remove_columns=['input_ids', 'token_type_ids', 'attention_mask']\n",
    ")\n",
    "\n",
    "# Due to the varchar constraint we are going to limit the question size when inserting\n",
    "def insert_function(batch):\n",
    "    insertable = [\n",
    "        {\n",
    "            'original_question': x,\n",
    "            'answer': batch['answer'][i],\n",
    "            'original_question_embedding': batch['question_embedding'].tolist()[i]\n",
    "        } for i, x in enumerate(batch['question'])\n",
    "    ]\n",
    "\n",
    "    collection.insert(data=insertable)\n",
    "\n",
    "data_dataset.map(insert_function, batched=True, batch_size=64)\n",
    "\n",
    "# Flush the data to disk \n",
    "# Zilliz Cloud automatically flushes the data to disk once a segment is full. \n",
    "# You do not always need to call this method.\n",
    "collection.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask questions\n",
    "\n",
    "Once all the data is inserted and indexed within Zilliz Cloud, we can ask questions and see what the closest answers are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2/2 [00:00<00:00, 27.43 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00,  3.70 examples/s]\n",
      "Map:   0%|          | 0/2 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 5 named id expected length 1 but got length 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/zilliz/zdoc-demos/python/81_integrations_huggingface.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zilliz/zdoc-demos/python/81_integrations_huggingface.ipynb#X14sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m         overall_original_question\u001b[39m.\u001b[39mappend(original_question)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zilliz/zdoc-demos/python/81_integrations_huggingface.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zilliz/zdoc-demos/python/81_integrations_huggingface.ipynb#X14sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m: overall_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zilliz/zdoc-demos/python/81_integrations_huggingface.ipynb#X14sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mdistance\u001b[39m\u001b[39m'\u001b[39m: overall_distance,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zilliz/zdoc-demos/python/81_integrations_huggingface.ipynb#X14sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m: overall_answer,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zilliz/zdoc-demos/python/81_integrations_huggingface.ipynb#X14sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39moriginal_question\u001b[39m\u001b[39m'\u001b[39m: overall_original_question\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zilliz/zdoc-demos/python/81_integrations_huggingface.ipynb#X14sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     }\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zilliz/zdoc-demos/python/81_integrations_huggingface.ipynb#X14sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m question_dataset \u001b[39m=\u001b[39m question_dataset\u001b[39m.\u001b[39;49mmap(search, batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, batch_size \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zilliz/zdoc-demos/python/81_integrations_huggingface.ipynb#X14sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m question_dataset:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zilliz/zdoc-demos/python/81_integrations_huggingface.ipynb#X14sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39mprint\u001b[39m()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/datasets/arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    593\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    594\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    558\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/datasets/arrow_dataset.py:3097\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3090\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3091\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   3092\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3093\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3094\u001b[0m         total\u001b[39m=\u001b[39mpbar_total,\n\u001b[1;32m   3095\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3096\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3097\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3098\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   3099\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/datasets/arrow_dataset.py:3493\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3491\u001b[0m         writer\u001b[39m.\u001b[39mwrite_table(pa\u001b[39m.\u001b[39mTable\u001b[39m.\u001b[39mfrom_pandas(batch))\n\u001b[1;32m   3492\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3493\u001b[0m         writer\u001b[39m.\u001b[39;49mwrite_batch(batch)\n\u001b[1;32m   3494\u001b[0m num_examples_progress_update \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m num_examples_in_batch\n\u001b[1;32m   3495\u001b[0m \u001b[39mif\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m _time \u001b[39m+\u001b[39m config\u001b[39m.\u001b[39mPBAR_REFRESH_TIME_INTERVAL:\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/datasets/arrow_writer.py:558\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[0;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[1;32m    556\u001b[0m         inferred_features[col] \u001b[39m=\u001b[39m typed_sequence\u001b[39m.\u001b[39mget_inferred_type()\n\u001b[1;32m    557\u001b[0m schema \u001b[39m=\u001b[39m inferred_features\u001b[39m.\u001b[39marrow_schema \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpa_writer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema\n\u001b[0;32m--> 558\u001b[0m pa_table \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39;49mTable\u001b[39m.\u001b[39;49mfrom_arrays(arrays, schema\u001b[39m=\u001b[39;49mschema)\n\u001b[1;32m    559\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/pyarrow/table.pxi:3674\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/pyarrow/table.pxi:2837\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.validate\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/pyarrow/error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Column 5 named id expected length 1 but got length 0"
     ]
    }
   ],
   "source": [
    "questions = {'question':['When did the premier league start?', 'Where did people learn russian?']}\n",
    "question_dataset = Dataset.from_dict(questions)\n",
    "\n",
    "question_dataset = question_dataset.map(tokenize_question, batched = True, batch_size=TOKENIZATION_BATCH_SIZE)\n",
    "question_dataset.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask'], output_all_columns=True)\n",
    "question_dataset = question_dataset.map(embed, remove_columns=['input_ids', 'token_type_ids', 'attention_mask'], batched = True, batch_size=INFERENCE_BATCH_SIZE)\n",
    "\n",
    "def search(batch):\n",
    "    res = collection.search(\n",
    "        data=batch['question_embedding'].tolist(),\n",
    "        anns_field='original_question_embedding',\n",
    "        param={\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}},\n",
    "        output_fields=['answer', 'original_question'], \n",
    "        limit = LIMIT\n",
    "    )\n",
    "    overall_id = []\n",
    "    overall_distance = []\n",
    "    overall_answer = []\n",
    "    overall_original_question = []\n",
    "    for hits in res:\n",
    "        ids = []\n",
    "        distance = []\n",
    "        answer = []\n",
    "        original_question = []\n",
    "        for hit in hits:\n",
    "            ids.append(hit['id'])\n",
    "            distance.append(hit['distance'])\n",
    "            answer.append(hit['entity']['answer'])\n",
    "            original_question.append(hit['entity']['original_question'])\n",
    "        overall_id.append(ids)\n",
    "        overall_distance.append(distance)\n",
    "        overall_answer.append(answer)\n",
    "        overall_original_question.append(original_question)\n",
    "    return {\n",
    "        'id': overall_id,\n",
    "        'distance': overall_distance,\n",
    "        'answer': overall_answer,\n",
    "        'original_question': overall_original_question\n",
    "    }\n",
    "question_dataset = question_dataset.map(search, batched=True, batch_size = 1)\n",
    "for x in question_dataset:\n",
    "    print()\n",
    "    print('Question:')\n",
    "    print(x['question'])\n",
    "    print('Answer, Distance, Original Question')\n",
    "    for x in zip(x['answer'], x['distance'], x['original_question']):\n",
    "        print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
