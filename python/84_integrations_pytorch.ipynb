{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Search with Zilliz Cloud and PyTorch\n",
    "\n",
    "On this page, we are going to go over a simple image search example using Zilliz Cloud. The dataset we are searching through is the Impressionist-Classifier Dataset found on [Kaggle](https://www.kaggle.com/datasets/delayedkarma/impressionist-classifier-data). For this example, we have re-hosted the data in a public google drive.\n",
    "\n",
    "For this example, we are just using a 1 CU cluster and using the Torchvision pre-trained ResNet50 model for embeddings. Let's get started!\n",
    "\n",
    "## Before you start\n",
    "\n",
    "For this example, we are going to use **pymilvus** to connect to Zilliz Cloud, **torch** to run the embedding model, **torchvision** for the actual model and preprocessing, **gdown** to download the example dataset and **tqdm** for loading bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymilvus torch gdown torchvision tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown \n",
    "import zipfile\n",
    "import glob\n",
    "import torch\n",
    "import time\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from pymilvus import MilvusClient, DataType, CollectionSchema, FieldSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "We are going to use gdown to grab the zip from Google Drive and then decompress it with the built-in zipfile library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://drive.google.com/uc?id=1OYDHLEy992qu5C4C8HV5uDIkOWRTAR1_'\n",
    "output = './paintings.zip'\n",
    "gdown.download(url, output)\n",
    "\n",
    "with zipfile.ZipFile(\"./paintings.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./paintings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "These are some of the main global arguments that we will be using for easier tracking and updating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set up the name of the collection to be created.\n",
    "COLLECTION_NAME = 'image_search_db'\n",
    "\n",
    "# 2. Set up the dimension of the embeddings.\n",
    "DIMENSION = 2048\n",
    "\n",
    "# 3. Set the inference parameters\n",
    "BATCH_SIZE = 128\n",
    "TOP_K = 3\n",
    "\n",
    "# 4. Set up the connection parameters for your Zilliz Cloud cluster.\n",
    "URI = 'https://in03-24426a264d9129a.api.gcp-us-west1.zillizcloud.com'\n",
    "\n",
    "# For serverless clusters, use your API key as the token.\n",
    "# For dedicated clusters, use a colon (:) concatenating your username and password as the token.\n",
    "TOKEN = 'e90b8dfdf09dce948b254f0d238324143e65ced62e469aed3bfe56c474d39d3f0a0e270c599ab6acb23ffe1aa2972d1adf6d4827'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Zilliz Cloud\n",
    "\n",
    "At this point, we are going to begin setting up Zilliz Cloud. The steps are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Zilliz Cloud and create a collection\n",
    "client = MilvusClient(uri=URI, token=TOKEN)\n",
    "\n",
    "if COLLECTION_NAME in client.list_collections():\n",
    "    client.drop_collection(COLLECTION_NAME)\n",
    "\n",
    "fields = [\n",
    "    FieldSchema(name='id', dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name='filepath', dtype=DataType.VARCHAR, max_length=200),  # VARCHARS need a maximum length, so for this example they are set to 200 characters\n",
    "    FieldSchema(name='image_embedding', dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)\n",
    "]\n",
    "\n",
    "schema = CollectionSchema(fields=fields)\n",
    "\n",
    "index_params = {\n",
    "    'index_type': 'AUTOINDEX',\n",
    "    'metric_type': 'L2',\n",
    "    'params': {}\n",
    "}\n",
    "\n",
    "client.create_collection_with_schema(\n",
    "    collection_name=COLLECTION_NAME, \n",
    "    schema=schema, \n",
    "    index_params=index_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert data\n",
    "\n",
    "In this example, we will use the ResNet50 model from the torch library and its model hub. To obtain embeddings, we will remove the final classification layer, resulting in the model providing embeddings of 2048 dimensions. All vision models found on **torch** use the same preprocessing method, which we have included here.\n",
    "\n",
    "In the following steps, we will:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the filepaths of the images\n",
    "paths = glob.glob('../paintings/paintings/**/*.jpg', recursive=True)\n",
    "print(len(paths))\n",
    "\n",
    "# Load the embedding model with the last layer removed\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "model.eval()\n",
    "\n",
    "# Preprocessing for images\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Embed function that embeds the batch and inserts it\n",
    "def embed(data):\n",
    "    with torch.no_grad():\n",
    "        output = model(torch.stack(data[0])).squeeze()\n",
    "        data = [ {\n",
    "            'filepath': x[0],\n",
    "            'image_embedding': x[1]\n",
    "        } for x in zip(data[1], output.tolist()) ]\n",
    "        client.insert(COLLECTION_NAME, data)\n",
    "\n",
    "data_batch = [[],[]]\n",
    "\n",
    "# Read the images into batches for embedding and insertion\n",
    "for path in tqdm(paths):\n",
    "    im = Image.open(path).convert('RGB')\n",
    "    data_batch[0].append(preprocess(im))\n",
    "    data_batch[1].append(path)\n",
    "    if len(data_batch[0]) % BATCH_SIZE == 0:\n",
    "        embed(data_batch)\n",
    "        data_batch = [[],[]]\n",
    "\n",
    "# Embed and insert the remainder\n",
    "if len(data_batch[0]) != 0:\n",
    "    embed(data_batch)\n",
    "\n",
    "# Call a flush to index any unsealed segments.\n",
    "client.flush(COLLECTION_NAME)\n",
    "\n",
    "# Get the filepaths of the search images\n",
    "search_paths = glob.glob('./paintings/test_paintings/**/*.jpg', recursive=True)\n",
    "len(search_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform search\n",
    "\n",
    "With all the data inserted into Zilliz Cloud, we can start performing our searches. In this example, we are going to search for two example images. Because we are doing a batch search, the search time is shared across the images of the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the search images\n",
    "def embed(data):\n",
    "    with torch.no_grad():\n",
    "        ret = model(torch.stack(data))\n",
    "        # If more than one image, use squeeze\n",
    "        if len(ret) > 1:\n",
    "            return ret.squeeze().tolist()\n",
    "        # Squeeze would remove batch for single image, so using flatten\n",
    "        else:\n",
    "            return torch.flatten(ret, start_dim=1).tolist()\n",
    "\n",
    "data_batch = [[],[]]\n",
    "\n",
    "for path in search_paths:\n",
    "    im = Image.open(path).convert('RGB')\n",
    "    data_batch[0].append(preprocess(im))\n",
    "    data_batch[1].append(path)\n",
    "\n",
    "embeds = embed(data_batch[0])\n",
    "start = time.time()\n",
    "res = client.search(COLLECTION_NAME, embeds, limit=TOP_K, output_fields=['filepath'])\n",
    "finish = time.time()\n",
    "\n",
    "# Show the image results\n",
    "f, axarr = plt.subplots(len(data_batch[1]), TOP_K + 1, figsize=(20, 10), squeeze=False)\n",
    "\n",
    "for hits_i, hits in enumerate(res):\n",
    "    axarr[hits_i][0].imshow(Image.open(data_batch[1][hits_i]))\n",
    "    axarr[hits_i][0].set_axis_off()\n",
    "    axarr[hits_i][0].set_title('Search Time: ' + str(finish - start))\n",
    "    for hit_i, hit in enumerate(hits):\n",
    "        axarr[hits_i][hit_i + 1].imshow(Image.open(hit['entity']['filepath']))\n",
    "        axarr[hits_i][hit_i + 1].set_axis_off()\n",
    "        axarr[hits_i][hit_i + 1].set_title('Distance: ' + str(hit['distance']))\n",
    "\n",
    "# Save the search result in a separate image file alongside your script.\n",
    "plt.savefig('search_result.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
