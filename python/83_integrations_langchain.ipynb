{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-augmented Generation with Zilliz Cloud and Langchain\n",
    "\n",
    "This guide demonstrates how to build an LLM-based Retrieval-augmented Generation using Zilliz Cloud and Langchain by setting up a question-answering application over a specific set of documents.\n",
    "\n",
    "For this example, we will use a 1 CU cluster from Zilliz Cloud.\n",
    "\n",
    "## Before you start\n",
    "\n",
    "Code snippets on this page require **pymilvus** and **langchain** to be installed. Additionally, OpenAI's embedding API has been used to embed documents into the vector store, so **openai** and **tiktoken** are also required. If these packages are not already installed on your system, run the following commands to install them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pymilvus langchain openai tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dependencies are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.zilliz import Zilliz\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "This section outlines the necessary steps to set up parameters for the code snippets that follow. Replace the default values with your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up arguments\n",
    "\n",
    "# 1. Set up the name of the collection to be created.\n",
    "COLLECTION_NAME = 'doc_qa_db'\n",
    "\n",
    "# 2. Set up the dimension of the embeddings.\n",
    "DIMENSION = 768\n",
    "\n",
    "# 3. Set up the cohere api key\n",
    "OPENAI_API_KEY = \"YOUR_OPENAI_API_KEY\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# 4. Set up the connection parameters for your Zilliz Cloud cluster.\n",
    "URI = 'YOUR_CLUSTER_ENDPOINT'\n",
    "\n",
    "# For serverless clusters, use your API key as the token.\n",
    "# For dedicated clusters, use a colon (:) concatenating your username and password as the token.\n",
    "TOKEN = 'YOUR_CLUSTER_TOKEN'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "Before diving in, complete the following steps:\n",
    "\n",
    "- Load documents and split them at a proper size.\n",
    "\n",
    "- Embed the splits and store them in a vector store.\n",
    "\n",
    "- Edit the prompt and raise the questions.\n",
    "\n",
    "### Load documents\n",
    "\n",
    "In this section, we are using the **WebBaseLoader** to load online pages from [milvus.com](https://milvus.com), and split them into chunks with a proper size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the WebBaseLoader to load specified web pages into documents\n",
    "loader = WebBaseLoader([\n",
    "    'https://milvus.io/docs/overview.md',\n",
    "    'https://milvus.io/docs/release_notes.md',\n",
    "    'https://milvus.io/docs/architecture_overview.md',\n",
    "    'https://milvus.io/docs/four_layers.md',\n",
    "    'https://milvus.io/docs/main_components.md',\n",
    "    'https://milvus.io/docs/data_processing.md',\n",
    "    'https://milvus.io/docs/bitset.md',\n",
    "    'https://milvus.io/docs/boolean.md',\n",
    "    'https://milvus.io/docs/consistency.md',\n",
    "    'https://milvus.io/docs/coordinator_ha.md',\n",
    "    'https://milvus.io/docs/replica.md',\n",
    "    'https://milvus.io/docs/knowhere.md',\n",
    "    'https://milvus.io/docs/schema.md',\n",
    "    'https://milvus.io/docs/dynamic_schema.md',\n",
    "    'https://milvus.io/docs/json_data_type.md',\n",
    "    'https://milvus.io/docs/metric.md',\n",
    "    'https://milvus.io/docs/partition_key.md',\n",
    "    'https://milvus.io/docs/multi_tenancy.md',\n",
    "    'https://milvus.io/docs/timestamp.md',\n",
    "    'https://milvus.io/docs/users_and_roles.md',\n",
    "    'https://milvus.io/docs/index.md',\n",
    "    'https://milvus.io/docs/disk_index.md',\n",
    "    'https://milvus.io/docs/scalar_index.md',\n",
    "    'https://milvus.io/docs/performance_faq.md',\n",
    "    'https://milvus.io/docs/product_faq.md',\n",
    "    'https://milvus.io/docs/operational_faq.md',\n",
    "    'https://milvus.io/docs/troubleshooting.md',\n",
    "])\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# Split the documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed splits\n",
    "\n",
    "In this section, we are using OpenAI's embedding model to embed document segment splitted above, and stores them into a Zilliz Cloud cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alloc_timestamp unimplemented, ignore it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "connection_args = { 'uri': URI, 'token': TOKEN }\n",
    "\n",
    "vector_store = Zilliz(\n",
    "    embedding_function=embeddings, \n",
    "    connection_args=connection_args,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    drop_old=True,\n",
    ").from_documents(\n",
    "    all_splits,\n",
    "    embedding=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_args=connection_args,\n",
    ")\n",
    "\n",
    "query = \"What are the main components of Milvus?\"\n",
    "docs = vector_store.similarity_search(query)\n",
    "\n",
    "print(len(docs))\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0) \n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raise your question\n",
    "\n",
    "In this section, we customize a RAG chain and raise a question in our concern. You can change the question to the one you are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='IVF_FLAT is an index mechanism in Milvus that divides a vector space into clusters. It compares the distances between a target vector and the centers of all clusters to find the nearest clusters. Then, it compares the distances between the target vector and the vectors in the selected clusters to find the nearest vectors. IVF_FLAT demonstrates performance advantages when the number of vectors exceeds the value of nlist. Thanks for asking!')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use three sentences maximum and keep the answer as concise as possible. \n",
    "Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"Explain IVF_FLAT in Milvus.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
