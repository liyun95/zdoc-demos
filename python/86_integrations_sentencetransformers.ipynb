{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Search Using Zilliz Cloud and SentenceTransformers\n",
    "\n",
    "In this example, we are going to go over a Wikipedia article search using Zilliz Cloud and the SentenceTransformers library. The dataset we will search through is the Wikipedia-Movie-Plots Dataset found on [Kaggle](https://www.kaggle.com/datasets/jrobischon/wikipedia-movie-plots). For this example, we have re-hosted the data in a public Google drive.\n",
    "\n",
    "## Before you start\n",
    "\n",
    "For this example, we are going to be using **pymilvus** to connect to use Zilliz Cloud, **sentencetransformers** to generate vector embeddings, and **gdown** to download the example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pymilvus sentence-transformers gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Here we can find the main arguments that need to be modified for running with your own accounts. Beside each is a description of what it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown, zipfile, time, csv\n",
    "from tqdm import tqdm\n",
    "from pymilvus import connections, DataType, FieldSchema, CollectionSchema, Collection, utility\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Parameters for set up Zilliz Cloud\n",
    "COLLECTION_NAME = 'movies_db'  # Collection name\n",
    "DIMENSION = 384  # Embeddings size\n",
    "URI = 'http://localhost:19530'  # Endpoint URI obtained from Zilliz Cloud\n",
    "TOKEN = 'root:Milvus'  # API key or a colon-separated cluster username and password\n",
    "\n",
    "# Inference Arguments\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Search Arguments\n",
    "TOP_K = 3\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=11ISS45aO2ubNCGaC3Lvd3D7NT8Y7MeO8'\n",
    "output = '../movies.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=11ISS45aO2ubNCGaC3Lvd3D7NT8Y7MeO8\n",
      "From (redirected): https://drive.google.com/uc?id=11ISS45aO2ubNCGaC3Lvd3D7NT8Y7MeO8&confirm=t&uuid=ecb9fbc2-dbd9-4270-9f2a-ba538c62f05a\n",
      "To: /Users/anthony/Documents/Github/zdoc-demos/movies.zip\n",
      "100%|██████████| 30.9M/30.9M [00:02<00:00, 11.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "gdown.download(url, output)\n",
    "\n",
    "with zipfile.ZipFile(\"../movies.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"../movies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Zilliz Cloud\n",
    "\n",
    "At this point, we are going to begin setting up Zilliz Cloud. The steps are as follows:\n",
    "\n",
    "1. Connect to the Zilliz Cloud cluster using the provided URI.\n",
    "2. If the collection already exists, drop it.\n",
    "3. Create the collection that holds the id, title of the movie, and the embeddings of the plot text.\n",
    "4. Create an index on the newly created collection and load it into memory.\n",
    "\n",
    "Once these steps are done the collection is ready to be inserted into and searched. Any data added will be indexed automatically and be available for search immediately. If the data is very fresh, the search might be slower as brute force searching will be used on data that is still in process of getting indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Milvus Database\n",
    "connections.connect(\n",
    "    uri=URI, \n",
    "    token=TOKEN\n",
    ")\n",
    "\n",
    "# Remove any previous collections with the same name\n",
    "if utility.has_collection(COLLECTION_NAME):\n",
    "    utility.drop_collection(COLLECTION_NAME)\n",
    "\n",
    "\n",
    "# Create collection which includes the id, title, and embedding.\n",
    "fields = [\n",
    "    FieldSchema(name='id', dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name='title', dtype=DataType.VARCHAR, max_length=200),  # VARCHARS need a maximum length, so for this example they are set to 200 characters\n",
    "    FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)\n",
    "]\n",
    "schema = CollectionSchema(fields=fields)\n",
    "collection = Collection(name=COLLECTION_NAME, schema=schema)\n",
    "\n",
    "# Create an IVF_FLAT index for collection.\n",
    "index_params = {\n",
    "    'index_type': 'AUTOINDEX',\n",
    "    'metric_type': 'L2',\n",
    "    'params': {}\n",
    "}\n",
    "collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "collection.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert data\n",
    "\n",
    "For this example, we are going to use the SentenceTransformers miniLM model to create embeddings of the plot text. This model returns 384-dimensional embeddings.\n",
    "\n",
    "In these next few steps we will:\n",
    "\n",
    "1. Load the data.\n",
    "\n",
    "2. Embed the plot text data using SentenceTransformers.\n",
    "\n",
    "3. Insert the data into Zilliz Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34887/34887 [15:01<00:00, 38.70it/s]\n"
     ]
    }
   ],
   "source": [
    "transformer = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Extract the book titles\n",
    "def csv_load(file):\n",
    "    with open(file, newline='') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            if '' in (row[1], row[7]):\n",
    "                continue\n",
    "            yield (row[1], row[7])\n",
    "\n",
    "def count(file):\n",
    "    with open(file, newline='') as f:\n",
    "        count = 0\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            if '' in (row[1], row[7]):\n",
    "                continue\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# Extract embedding from text using OpenAI\n",
    "def embed_insert(data):\n",
    "    embeds = transformer.encode(data[1])\n",
    "    ins = [\n",
    "            data[0],\n",
    "            [x for x in embeds]\n",
    "    ]\n",
    "    collection.insert(ins)\n",
    "\n",
    "\n",
    "\n",
    "data_batch = [[],[]]\n",
    "\n",
    "for title, plot in tqdm(csv_load('../movies/plots.csv'), total=count('../movies/plots.csv')):\n",
    "    data_batch[0].append(title)\n",
    "    data_batch[1].append(plot)\n",
    "    if len(data_batch[0]) % BATCH_SIZE == 0:\n",
    "        embed_insert(data_batch)\n",
    "        data_batch = [[],[]]\n",
    "\n",
    "# Embed and insert the remainder\n",
    "if len(data_batch[0]) != 0:\n",
    "    embed_insert(data_batch)\n",
    "\n",
    "# Call a flush to index any unsealed segments.\n",
    "collection.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the search\n",
    "\n",
    "With all the data inserted into Zilliz Cloud, we can start performing our searches. In this example, we are going to search for movies based on the plot. Because we are doing a batch search, the search time is shared across the movie searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A movie about cars\n",
      "Search Time: 0.292910099029541\n",
      "Results:\n",
      "Auto Driver ---- 0.8420578837394714\n",
      "Auto Driver ---- 0.8420578837394714\n",
      "Red Line 7000 ---- 0.9104408025741577\n",
      "\n",
      "Title: A movie about monsters\n",
      "Search Time: 0.292910099029541\n",
      "Results:\n",
      "Monster Hunt ---- 0.8105474710464478\n",
      "Monster Hunt ---- 0.8105474710464478\n",
      "The Astro-Zombies ---- 0.8998499512672424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search for titles that closest match these phrases.\n",
    "search_terms = ['A movie about cars', 'A movie about monsters']\n",
    "\n",
    "# Search the database based on input text\n",
    "def embed_search(data):\n",
    "    embeds = transformer.encode(data)\n",
    "    return [x for x in embeds]\n",
    "\n",
    "search_data = embed_search(search_terms)\n",
    "\n",
    "start = time.time()\n",
    "res = collection.search(\n",
    "    data=search_data,  # Embeded search value\n",
    "    anns_field=\"embedding\",  # Search across embeddings\n",
    "    param={},\n",
    "    limit = TOP_K,  # Limit to top_k results per search\n",
    "    output_fields=['title']  # Include title field in result\n",
    ")\n",
    "end = time.time()\n",
    "\n",
    "for hits_i, hits in enumerate(res):\n",
    "    print('Title:', search_terms[hits_i])\n",
    "    print('Search Time:', end-start)\n",
    "    print('Results:')\n",
    "    for hit in hits:\n",
    "        print( hit.entity.get('title'), '----', hit.distance)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
