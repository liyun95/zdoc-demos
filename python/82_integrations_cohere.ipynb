{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxV-X75kBWLV"
      },
      "source": [
        "# Question Answering Using Zilliz Cloud and Cohere\n",
        "\n",
        "This page illustrates how to create a question-answering system based on the SQuAD dataset using Zilliz Cloud as the vector database and Cohere as the embedding system.\n",
        "\n",
        "## Before you start\n",
        "\n",
        "Code snippets on this page require pymilvus, cohere, pandas, numpy, and tqdm installed. Among these packages, pymilvus is the client for Zilliz Cloud. If these packages are not present on your system, run the following commands to install them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsxDrpQnBWLX",
        "outputId": "4a35c777-094a-4902-c70f-b1b166a9e0f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymilvus in /usr/local/lib/python3.10/dist-packages (2.3.3)\n",
            "Requirement already satisfied: cohere in /usr/local/lib/python3.10/dist-packages (4.34)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Collecting openai\n",
            "  Downloading openai-1.2.3-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio<=1.58.0,>=1.49.1 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (1.58.0)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (3.20.3)\n",
            "Requirement already satisfied: environs<=9.5.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (9.5.0)\n",
            "Requirement already satisfied: ujson>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (5.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pymilvus) (2.31.0)\n",
            "Requirement already satisfied: minio>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (7.2.0)\n",
            "Requirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (3.8.6)\n",
            "Requirement already satisfied: backoff<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.2.1)\n",
            "Requirement already satisfied: fastavro==1.8.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (1.8.2)\n",
            "Requirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (6.8.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: marshmallow>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from environs<=9.5.0->pymilvus) (3.20.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from environs<=9.5.0->pymilvus) (1.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from minio>=7.0.0->pymilvus) (23.1.0)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.10/dist-packages (from minio>=7.0.0->pymilvus) (3.19.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow>=3.0.0->environs<=9.5.0->pymilvus) (23.2)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->minio>=7.0.0->pymilvus) (21.2.0)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->minio>=7.0.0->pymilvus) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->minio>=7.0.0->pymilvus) (2.21)\n",
            "Installing collected packages: h11, tiktoken, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.1 openai-1.2.3 tiktoken-0.5.1\n"
          ]
        }
      ],
      "source": [
        "%pip install pymilvus cohere pandas numpy tqdm openai tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43nTMnZhBWLZ"
      },
      "source": [
        "Then you need to load the modules to be used in this guide."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PZYdXie-BWLZ"
      },
      "outputs": [],
      "source": [
        "from pymilvus import connections, DataType, CollectionSchema, FieldSchema, Collection, utility\n",
        "import cohere\n",
        "import pandas\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time, os, json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr5Kequ2BWLZ"
      },
      "source": [
        "## Parameters\n",
        "\n",
        "Here we can find the parameters used in the following snippets. Some of them need to be changed to fit your environment. Beside each is a description of what it is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lhmDx0p-BWLa"
      },
      "outputs": [],
      "source": [
        "# Set up arguments\n",
        "\n",
        "# 1. Set the The SQuAD dataset url.\n",
        "FILE = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json'\n",
        "\n",
        "# 2. Set up the name of the collection to be created.\n",
        "COLLECTION_NAME = 'question_answering_db'\n",
        "\n",
        "# 3. Set up the dimension of the embeddings.\n",
        "DIMENSION = 768\n",
        "\n",
        "# 4. Set the number of entities to create and the number of entities to insert at a time.\n",
        "COUNT = 5000\n",
        "BATCH_SIZE = 96\n",
        "\n",
        "# 5. Set up the cohere api key\n",
        "COHERE_API_KEY = \"YOUR_COHERE_API_KEY\"\n",
        "\n",
        "# 6. Set up the connection parameters for your Zilliz Cloud cluster.\n",
        "URI = 'YOUR_CLUSTER_ENDPOINT'\n",
        "TOKEN = 'YOUR_CLUSTER_TOKEN'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNYcceGfBWLa"
      },
      "source": [
        "To know more about the model and dataset used on this page, refer to [Cohere](https://cohere.ai/) and [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/).\n",
        "\n",
        "## Prepare dataset\n",
        "\n",
        "In this example, we are going to use the Stanford Question Answering Dataset (SQuAD) as our truth source for answering questions. This dataset comes in the form of a JSON file and we are going to use pandas to load it in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ON-MlKGFBWLa",
        "outputId": "bfcc800d-9d3c-416c-b2e0-d68da3da60f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5000\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "dataset = pandas.read_json(FILE)\n",
        "\n",
        "# Clean up the dataset by grabbing all the question answer pairs\n",
        "simplified_records = []\n",
        "for x in dataset['data']:\n",
        "    for y in x['paragraphs']:\n",
        "        for z in y['qas']:\n",
        "            if len(z['answers']) != 0:\n",
        "                simplified_records.append({'question': z['question'], 'answer': z['answers'][0]['text']})\n",
        "\n",
        "# Grab the amount of records based on COUNT\n",
        "simplified_records = pandas.DataFrame.from_records(simplified_records)\n",
        "simplified_records = simplified_records.sample(n=min(COUNT, len(simplified_records)), random_state = 42)\n",
        "\n",
        "# Check if the length of the cleaned dataset matches COUNT\n",
        "print(len(simplified_records))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1edGIbk8BWLb"
      },
      "source": [
        "## Create a collection\n",
        "\n",
        "This section deals with Zilliz Cloud and setting up the cluster for this use case. Within Zilliz Cloud, we need to set up a collection and index it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eANf8DaxBWLc"
      },
      "outputs": [],
      "source": [
        "# Connect to Zilliz Cloud and create a collection\n",
        "connections.connect(\n",
        "    alias='default',\n",
        "    # Public endpoint obtained from Zilliz Cloud\n",
        "    uri=URI,\n",
        "    token=TOKEN\n",
        ")\n",
        "\n",
        "if COLLECTION_NAME in utility.list_collections():\n",
        "    utility.drop_collection(COLLECTION_NAME)\n",
        "\n",
        "fields = [\n",
        "    FieldSchema(name='id', dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
        "    FieldSchema(name='original_question', dtype=DataType.VARCHAR, max_length=1000),\n",
        "    FieldSchema(name='answer', dtype=DataType.VARCHAR, max_length=1000),\n",
        "    FieldSchema(name='original_question_embedding', dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)\n",
        "]\n",
        "\n",
        "schema = CollectionSchema(fields=fields)\n",
        "\n",
        "collection = Collection(\n",
        "    name=COLLECTION_NAME,\n",
        "    schema=schema,\n",
        ")\n",
        "\n",
        "index_params = {\n",
        "    'metric_type': 'L2',\n",
        "    'index_type': 'AUTOINDEX',\n",
        "    'params': {}\n",
        "}\n",
        "\n",
        "collection.create_index(\n",
        "    field_name='original_question_embedding',\n",
        "    index_params=index_params\n",
        ")\n",
        "\n",
        "collection.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIbVDY3kBWLd"
      },
      "source": [
        "## Insert data\n",
        "\n",
        "Once we have the collection set up, we need to start inserting our data. This is done in three steps:\n",
        "\n",
        "- reading the data,\n",
        "\n",
        "- embedding the original questions, and\n",
        "\n",
        "- inserting the data into the collection we've just created on Zilliz Cloud.\n",
        "\n",
        "In this example, the data includes the original question, the original question's embedding, and the answer to the original question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjfbCrtNBWLd",
        "outputId": "5d17555f-215b-429c-f8a8-4abe9096309c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 53/53 [00:33<00:00,  1.56it/s]\n"
          ]
        }
      ],
      "source": [
        "# Set up a Cohere client\n",
        "cohere_client = cohere.Client(COHERE_API_KEY)\n",
        "\n",
        "# Extract embeddings from questions using Cohere\n",
        "def embed(texts, input_type):\n",
        "    res = cohere_client.embed(texts, model='multilingual-22-12', input_type=input_type)\n",
        "    return res.embeddings\n",
        "\n",
        "# Insert each question, answer, and qustion embedding\n",
        "total = pandas.DataFrame()\n",
        "for batch in tqdm(np.array_split(simplified_records, (COUNT/BATCH_SIZE) + 1)):\n",
        "    questions = batch['question'].tolist()\n",
        "    embeddings = embed(questions, \"search_document\")\n",
        "\n",
        "    data = [\n",
        "        {\n",
        "            'original_question': x,\n",
        "            'answer': batch['answer'].tolist()[i],\n",
        "            'original_question_embedding': embeddings[i]\n",
        "        } for i, x in enumerate(questions)\n",
        "    ]\n",
        "\n",
        "    collection.insert(data=data)\n",
        "\n",
        "time.sleep(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ksZziFvBWLd"
      },
      "source": [
        "## Ask questions\n",
        "\n",
        "Once all the data is inserted into the Zilliz Cloud collection, we can ask the system questions by taking our question phrase, embedding it with Cohere, and searching with Zilliz Cloud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ehNjvbVBWLd",
        "outputId": "68b36f6b-76ec-40ab-d108-346ccb7ba3a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "    {\n",
            "        \"question\": \"What kills bacteria?\",\n",
            "        \"candidates\": [\n",
            "            {\n",
            "                \"answer\": \"farming\",\n",
            "                \"distance\": 25.10422134399414,\n",
            "                \"original_question\": \"What makes bacteria resistant to antibiotic treatment?\"\n",
            "            },\n",
            "            {\n",
            "                \"answer\": \"converting nitrogen gas to nitrogenous compounds\",\n",
            "                \"distance\": 25.26958465576172,\n",
            "                \"original_question\": \"What do bacteria do in soil?\"\n",
            "            },\n",
            "            {\n",
            "                \"answer\": \"slowing down the multiplication of bacteria or killing the bacteria\",\n",
            "                \"distance\": 26.225540161132812,\n",
            "                \"original_question\": \"How do antibiotics work?\"\n",
            "            },\n",
            "            {\n",
            "                \"answer\": \"Phage therapy\",\n",
            "                \"distance\": 30.04580307006836,\n",
            "                \"original_question\": \"What has been talked about to treat resistant bacteria?\"\n",
            "            },\n",
            "            {\n",
            "                \"answer\": \"antibiotic target\",\n",
            "                \"distance\": 32.077369689941406,\n",
            "                \"original_question\": \"What can be absent from the bacterial genome?\"\n",
            "            }\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"question\": \"What's the biggest dog?\",\n",
            "        \"candidates\": [\n",
            "            {\n",
            "                \"answer\": \"English Mastiff\",\n",
            "                \"distance\": 12.71607780456543,\n",
            "                \"original_question\": \"What breed was the largest dog known to have lived?\"\n",
            "            },\n",
            "            {\n",
            "                \"answer\": \"part of the family\",\n",
            "                \"distance\": 27.21062469482422,\n",
            "                \"original_question\": \"Most people today describe their dogs as what?\"\n",
            "            },\n",
            "            {\n",
            "                \"answer\": \"77.5 million\",\n",
            "                \"distance\": 28.54041290283203,\n",
            "                \"original_question\": \"How many people in the United States are said to own dog?\"\n",
            "            },\n",
            "            {\n",
            "                \"answer\": \"Rico\",\n",
            "                \"distance\": 28.770610809326172,\n",
            "                \"original_question\": \"What is the name of the dog that could ID over 200 things?\"\n",
            "            },\n",
            "            {\n",
            "                \"answer\": \"about six\",\n",
            "                \"distance\": 31.739566802978516,\n",
            "                \"original_question\": \"What is the average number of pups in a litter?\"\n",
            "            }\n",
            "        ]\n",
            "    }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# Search the cluster for an answer to a question text\n",
        "def search(text, top_k = 5):\n",
        "\n",
        "    # AUTOINDEX does not require any search params\n",
        "    search_params = {}\n",
        "\n",
        "    results = collection.search(\n",
        "        data = embed([text], \"search_query\"),  # Embeded the question\n",
        "        anns_field='original_question_embedding',\n",
        "        param=search_params,\n",
        "        limit = top_k,  # Limit to top_k results per search\n",
        "        output_fields=['original_question', 'answer']  # Include the original question and answer in the result\n",
        "    )\n",
        "\n",
        "    distances = results[0].distances\n",
        "    entities = [ x.entity.to_dict()['entity'] for x in results[0] ]\n",
        "\n",
        "    ret = [ {\n",
        "        \"answer\": x[1][\"answer\"],\n",
        "        \"distance\": x[0],\n",
        "        \"original_question\": x[1]['original_question']\n",
        "    } for x in zip(distances, entities)]\n",
        "\n",
        "    return ret\n",
        "\n",
        "\n",
        "# Ask these questions\n",
        "search_questions = ['What kills bacteria?', 'What\\'s the biggest dog?']\n",
        "\n",
        "# Print out the results in order of [answer, similarity score, original question]\n",
        "\n",
        "ret = [ { \"question\": x, \"candidates\": search(x) } for x in search_questions ]\n",
        "\n",
        "print(json.dumps(ret, indent=4))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
