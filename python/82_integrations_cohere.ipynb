{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering Using Zilliz Cloud and Cohere\n",
    "\n",
    "This page illustrates how to create a question-answering system based on the SQuAD dataset using Zilliz Cloud as the vector database and Cohere as the embedding system.\n",
    "\n",
    "## Before you start\n",
    "\n",
    "Code snippets on this page require pymilvus, cohere, pandas, numpy, and tqdm installed. Among these packages, pymilvus is the client for Zilliz Cloud. If these packages are not present on your system, run the following commands to install them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymilvus cohere pandas numpy tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you need to load the modules to be used in this guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PyMilvus in development\n",
    "# Should be replaced with `from pymilvus import *` in production\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path(\"/Users/anthony/Documents/play/refine_milvus/pymilvus\")))\n",
    "\n",
    "from pymilvus import MilvusClient, DataType, CollectionSchema, FieldSchema\n",
    "import cohere\n",
    "import pandas\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Here we can find the parameters used in the following snippets. Some of them need to be changed to fit your environment. Beside each is a description of what it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up arguments\n",
    "\n",
    "# 1. Set the The SQuAD dataset url.\n",
    "FILE = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json' \n",
    "\n",
    "# 2. Set up the name of the collection to be created.\n",
    "COLLECTION_NAME = 'question_answering_db'\n",
    "\n",
    "# 3. Set up the dimension of the embeddings.\n",
    "DIMENSION = 768\n",
    "\n",
    "# 4. Set the number of entities to create and the number of entities to insert at a time.\n",
    "COUNT = 5000\n",
    "BATCH_SIZE = 96\n",
    "\n",
    "# 5. Set up the cohere api key\n",
    "COHERE_API_KEY = \"COHERE_API_KEY\"\n",
    "\n",
    "# 6. Set up the connection parameters for your Zilliz Cloud cluster.\n",
    "URI = 'YOUR_CLUSTER_ENDPOINT'\n",
    "\n",
    "# For serverless clusters, use your API key as the token.\n",
    "# For dedicated clusters, use a colon (:) concatenating your username and password as the token.\n",
    "TOKEN = 'YOUR_CLUSTER_TOKEN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know more about the model and dataset used on this page, refer to [Cohere](https://cohere.ai/) and [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/).\n",
    "\n",
    "## Prepare dataset\n",
    "\n",
    "In this example, we are going to use the Stanford Question Answering Dataset (SQuAD) as our truth source for answering questions. This dataset comes in the form of a JSON file and we are going to use pandas to load it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "dataset = pandas.read_json(FILE)\n",
    "\n",
    "# Clean up the dataset by grabbing all the question answer pairs\n",
    "simplified_records = []\n",
    "for x in dataset['data']:\n",
    "    for y in x['paragraphs']:\n",
    "        for z in y['qas']:\n",
    "            if len(z['answers']) != 0:\n",
    "                simplified_records.append({'question': z['question'], 'answer': z['answers'][0]['text']})\n",
    "\n",
    "# Grab the amount of records based on COUNT\n",
    "simplified_records = pandas.DataFrame.from_records(simplified_records)\n",
    "simplified_records = simplified_records.sample(n=min(COUNT, len(simplified_records)), random_state = 42)\n",
    "\n",
    "# Check if the length of the cleaned dataset matches COUNT\n",
    "print(len(simplified_records))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a collection\n",
    "\n",
    "This section deals with Zilliz Cloud and setting up the cluster for this use case. Within Zilliz Cloud, we need to set up a collection and index it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alloc_timestamp unimplemented, ignore it\n"
     ]
    }
   ],
   "source": [
    "# Connect to Zilliz Cloud and create a collection\n",
    "\n",
    "client = MilvusClient(uri=URI, token=TOKEN)\n",
    "\n",
    "if COLLECTION_NAME in client.list_collections():\n",
    "    client.drop_collection(COLLECTION_NAME)\n",
    "\n",
    "fields = [\n",
    "    FieldSchema(name='id', dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name='original_question', dtype=DataType.VARCHAR, max_length=1000),\n",
    "    FieldSchema(name='answer', dtype=DataType.VARCHAR, max_length=1000),\n",
    "    FieldSchema(name='original_question_embedding', dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)\n",
    "]\n",
    "\n",
    "schema = CollectionSchema(fields=fields)\n",
    "\n",
    "index_params = {\n",
    "    'metric_type': 'L2',\n",
    "    'index_type': 'AUTOINDEX',\n",
    "    'params': {'nlist': 1024}\n",
    "}\n",
    "\n",
    "client.create_collection_with_schema(\n",
    "    collection_name=COLLECTION_NAME, \n",
    "    schema=schema, \n",
    "    index_params=index_params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert data\n",
    "\n",
    "Once we have the collection set up, we need to start inserting our data. This is done in three steps:\n",
    "\n",
    "- reading the data,\n",
    "\n",
    "- embedding the original questions, and\n",
    "\n",
    "- inserting the data into the collection we've just created on Zilliz Cloud.\n",
    "\n",
    "In this example, the data includes the original question, the original question's embedding, and the answer to the original question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthony/Documents/Github/zdoc-demos/python/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "100%|██████████| 53/53 [01:41<00:00,  1.91s/it]\n"
     ]
    }
   ],
   "source": [
    "# Set up a Cohere client\n",
    "cohere_client = cohere.Client(COHERE_API_KEY)\n",
    "\n",
    "# Extract embeddings from questions using Cohere\n",
    "def embed(texts):\n",
    "    res = cohere_client.embed(texts, model='multilingual-22-12')\n",
    "    return res.embeddings\n",
    "\n",
    "# Insert each question, answer, and qustion embedding\n",
    "total = pandas.DataFrame()\n",
    "for batch in tqdm(np.array_split(simplified_records, (COUNT/BATCH_SIZE) + 1)):\n",
    "    questions = batch['question'].tolist()\n",
    "    embeddings = embed(questions)\n",
    "    \n",
    "    data = [\n",
    "        {\n",
    "            'original_question': x,\n",
    "            'answer': batch['answer'].tolist()[i],\n",
    "            'original_question_embedding': embeddings[i]\n",
    "        } for i, x in enumerate(questions)\n",
    "    ]\n",
    "\n",
    "    client.insert(collection_name=COLLECTION_NAME, data=data)\n",
    "\n",
    "# Flush at end to make sure all rows are sent for indexing\n",
    "client.flush(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask questions\n",
    "\n",
    "Once all the data is inserted into the Zilliz Cloud collection, we can ask the system questions by taking our question phrase, embedding it with Cohere, and searching with Zilliz Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What kills bacteria?\n",
      "\n",
      "Answer, Distance, Original Question\n",
      "['farming', 25.131155014038086, 'What makes bacteria resistant to antibiotic treatment?']\n",
      "['converting nitrogen gas to nitrogenous compounds', 25.32603645324707, 'What do bacteria do in soil?']\n",
      "['slowing down the multiplication of bacteria or killing the bacteria', 26.2572021484375, 'How do antibiotics work?']\n",
      "['Phage therapy', 30.06584358215332, 'What has been talked about to treat resistant bacteria?']\n",
      "['antibiotic target', 32.11707305908203, 'What can be absent from the bacterial genome?']\n",
      "\n",
      "Question: What's the biggest dog?\n",
      "\n",
      "Answer, Distance, Original Question\n",
      "['English Mastiff', 12.715330123901367, 'What breed was the largest dog known to have lived?']\n",
      "['part of the family', 27.195798873901367, 'Most people today describe their dogs as what?']\n",
      "['77.5 million', 28.52544403076172, 'How many people in the United States are said to own dog?']\n",
      "['Rico', 28.769195556640625, 'What is the name of the dog that could ID over 200 things?']\n",
      "['about six', 31.727481842041016, 'What is the average number of pups in a litter?']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search the cluster for an answer to a question text\n",
    "def search(text, top_k = 5):\n",
    "\n",
    "    # AUTOINDEX does not require any search params \n",
    "    search_params = {}\n",
    "\n",
    "    results = client.search(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        data = embed([text]),  # Embeded the question\n",
    "        limit = top_k,  # Limit to top_k results per search\n",
    "        output_fields=['original_question', 'answer']  # Include the original question and answer in the result\n",
    "    )\n",
    "\n",
    "    ret = []\n",
    "    for hit in results[0]:\n",
    "        row = []\n",
    "        row.extend([hit['entity']['answer'], hit['distance'], hit['entity']['original_question'] ])  # Get the answer, distance, and original question for the results\n",
    "        ret.append(row)\n",
    "    return ret\n",
    "\n",
    "# Ask these questions\n",
    "search_questions = ['What kills bacteria?', 'What\\'s the biggest dog?']\n",
    "\n",
    "# Print out the results in order of [answer, similarity score, original question]\n",
    "for question in search_questions:\n",
    "    print('Question:', question)\n",
    "    print('\\nAnswer,', 'Distance,', 'Original Question')\n",
    "    for result in search(question):\n",
    "        print(result)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
